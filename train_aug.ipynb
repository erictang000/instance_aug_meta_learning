{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from metadatas import *\n",
    "from augmentations import *\n",
    "\n",
    "from models.classification_heads import ClassificationHead, R2D2Head\n",
    "from models.classification_heads import ClassificationHead_Mixup, R2D2Head_Mixup\n",
    "from models.R2D2_embedding import R2D2Embedding\n",
    "from models.R2D2_embedding_mixup import R2D2Embedding_mixup\n",
    "from models.protonet_embedding import ProtoNetEmbedding\n",
    "from models.ResNet12_embedding import resnet12 \n",
    "from models.ResNet12_embedding_mixup import resnet12_mixup \n",
    "from InstaAug_module import learnable_invariance\n",
    "\n",
    "from utils import set_gpu, Timer, count_accuracy, count_accuracy_mixup, check_dir, log\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import time\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "\n",
    "#np.seterr(all='raise')\n",
    "\n",
    "def mixup_data(x, y, lam, use_cuda=False):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).to(DEVICE)\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    loss = 0.\n",
    "    for i in range(len(pred)):\n",
    "        loss += lam[i] * criterion(pred[i], y_a[i]) + (1 - lam[i]) * criterion(pred[i], y_b[i])\n",
    "\n",
    "    return loss/len(pred)\n",
    "\n",
    "\n",
    "def one_hot(indices, depth):\n",
    "    \"\"\"\n",
    "    Returns a one-hot tensor.\n",
    "    This is a PyTorch equivalent of Tensorflow's tf.one_hot.\n",
    "        \n",
    "    Parameters:\n",
    "      indices:  a (n_batch, m) Tensor or (m) Tensor.\n",
    "      depth: a scalar. Represents the depth of the one hot dimension.\n",
    "    Returns: a (n_batch, m, depth) Tensor or (m, depth) Tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).to(DEVICE)\n",
    "    index = indices.view(indices.size()+torch.Size([1]))\n",
    "    encoded_indicies = encoded_indicies.scatter_(1,index,1)\n",
    "    \n",
    "    return encoded_indicies\n",
    "\n",
    "def get_model(options):\n",
    "    # Choose the embedding network\n",
    "    if options.network == 'ProtoNet':\n",
    "        network = ProtoNetEmbedding().to(DEVICE)\n",
    "    elif options.network == 'R2D2':\n",
    "        network = R2D2Embedding().to(DEVICE)\n",
    "    elif options.network == 'R2D2_mixup':\n",
    "        network = R2D2Embedding_mixup().to(DEVICE)\n",
    "    elif options.network == 'ResNet_mixup':\n",
    "        network = resnet12_mixup(avg_pool=False, drop_rate=0.1, dropblock_size=2).to(DEVICE)\n",
    "    elif options.network == 'ResNet':\n",
    "        if options.dataset == 'miniImageNet' or options.dataset == 'tieredImageNet':\n",
    "            network = resnet12(avg_pool=False, drop_rate=0.1, dropblock_size=4).to(DEVICE)\n",
    "            network = torch.nn.DataParallel(network)\n",
    "        else:\n",
    "            network = resnet12(avg_pool=False, drop_rate=0.1, dropblock_size=2).to(DEVICE)\n",
    "            network = torch.nn.DataParallel(network)\n",
    "    else:\n",
    "        print (\"Cannot recognize the network type\")\n",
    "        assert(False)\n",
    "        \n",
    "    # Choose the classification head\n",
    "    if options.head == 'ProtoNet':\n",
    "        cls_head = ClassificationHead(base_learner='ProtoNet').to(DEVICE)\n",
    "    elif options.head == 'Ridge':\n",
    "        cls_head = ClassificationHead(base_learner='Ridge').to(DEVICE)\n",
    "    elif options.head == 'R2D2':\n",
    "        cls_head = R2D2Head().to(DEVICE) \n",
    "    elif options.head == 'SVM':\n",
    "        cls_head = ClassificationHead(base_learner='SVM-CS').to(DEVICE)\n",
    "    else:\n",
    "        print (\"Cannot recognize the dataset type\")\n",
    "        assert(False)\n",
    "        \n",
    "    if options.support_aug and 'mix' in options.support_aug:\n",
    "        if options.head == 'R2D2':\n",
    "            cls_head_mixup = R2D2Head_Mixup().to(DEVICE)\n",
    "        elif options.head == 'SVM':\n",
    "            cls_head_mixup = ClassificationHead_Mixup(base_learner='SVM-CS').to(DEVICE)\n",
    "        else:\n",
    "            print(\"Cannot recognize the dataset type\")\n",
    "\n",
    "        return (network, cls_head, cls_head_mixup)\n",
    "        \n",
    "    else:\n",
    "        return (network, cls_head)\n",
    "\n",
    "\n",
    "def get_datasets(name, phase, args):\n",
    "    random_cropping = False if args.no_random_cropping else True\n",
    "    color_jitter = False if args.no_color_jitter else True\n",
    "    \n",
    "    if name == 'miniImageNet':\n",
    "        dataset = MiniImageNet(phase=phase, augment=args.feat_aug, rot90_p=args.t_p, random_cropping=random_cropping, color_jitter=color_jitter)  \n",
    "    elif name == 'CIFAR_FS':\n",
    "        dataset = CIFAR_FS(phase=phase, augment=args.feat_aug, rot90_p=args.t_p, random_cropping=random_cropping, color_jitter=color_jitter)\n",
    "    # elif name == 'FC100':\n",
    "    #     dataset = FC100(phase=phase, augment=args.feat_aug, rot90_p=args.t_p)\n",
    "    else:\n",
    "        print (\"Cannot recognize the dataset type\")\n",
    "        assert(False)\n",
    "    print(dataset)\n",
    "\n",
    "    if phase == 'train':\n",
    "        for ta in args.task_aug:\n",
    "            if ta == 'Rot90':\n",
    "                dataset = Rot90(dataset, p=args.t_p, batch_size_down=8e4)\n",
    "                dataset.batch_num.value += args.num_epoch * 0.\n",
    "            elif ta == 'Mix':\n",
    "                dataset = TaskAug(dataset, \"Mix\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'CutMix':\n",
    "                dataset = TaskAug(dataset, \"CutMix\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'FMix':\n",
    "                dataset = TaskAug(dataset, \"FMix\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'Combine':\n",
    "                dataset = TaskAug(dataset, \"Combine\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'DropChannel':\n",
    "                dataset = DropChannels(dataset, p=args.t_p) \n",
    "            elif ta == 'RE':\n",
    "                dataset = RE(dataset, p=args.t_p) \n",
    "            elif ta == 'Solarize':\n",
    "                dataset = Solarize(dataset, p=args.t_p) \n",
    "            else:\n",
    "                print (\"Cannot recognize the task augmentation type\")\n",
    "                continue\n",
    "            print(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def apply_instance_aug(instance_aug_module, x, dataset_source=\"CIFAR_FS\"):\n",
    "    '''\n",
    "    Params:\n",
    "        instance_aug_module: torch.Module\n",
    "        x: input data\n",
    "        dataset source: str (either TinyImageNet or CIFAR_FS)\n",
    "\n",
    "    Returns:\n",
    "        instance augmentation applied on x\n",
    "    '''\n",
    "    return instance_aug_module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num-epoch', type=int, default=40,\n",
    "                        help='number of training epochs')\n",
    "parser.add_argument('--save-epoch', type=int, default=10,\n",
    "                        help='frequency of model saving')\n",
    "parser.add_argument('--train-shot', type=int, default=5,\n",
    "                        help='number of support examples per training class')\n",
    "parser.add_argument('--val-shot', type=int, default=5,\n",
    "                        help='number of support examples per validation class')\n",
    "parser.add_argument('--train-query', type=int, default=6,\n",
    "                        help='number of query examples per training class')\n",
    "parser.add_argument('--val-episode', type=int, default=2000,\n",
    "                        help='number of episodes per validation')\n",
    "parser.add_argument('--val-query', type=int, default=15,\n",
    "                        help='number of query examples per validation class')\n",
    "parser.add_argument('--train-way', type=int, default=5,\n",
    "                        help='number of classes in one training episode')\n",
    "parser.add_argument('--sample-way', type=int, default=5,\n",
    "                        help='number of classes sampled in one training episode (i.e. sample 10 classes for task mixup)')\n",
    "parser.add_argument('--test-way', type=int, default=5,\n",
    "                        help='number of classes in one test (or validation) episode')\n",
    "parser.add_argument('--save-path', default='./experiments/exp_1')\n",
    "parser.add_argument('--gpu', default='0, 1, 2, 3')\n",
    "parser.add_argument('--network', type=str, default='ProtoNet',\n",
    "                        help='choose which embedding network to use. ProtoNet, R2D2, ResNet')\n",
    "parser.add_argument('--head', type=str, default='ProtoNet',\n",
    "                        help='choose which classification head to use. ProtoNet, Ridge, R2D2, SVM')\n",
    "parser.add_argument('--dataset', type=str, default='miniImageNet',\n",
    "                        help='choose which classification head to use. miniImageNet, tieredImageNet, CIFAR_FS, FC100')\n",
    "parser.add_argument('--episodes-per-batch', type=int, default=8,\n",
    "                        help='number of episodes per batch')\n",
    "parser.add_argument('--num-per-batch', type=int, default=1000,\n",
    "                        help='number of epoch size per train epoch')\n",
    "parser.add_argument('--eps', type=float, default=0.0,\n",
    "                        help='epsilon of label smoothing')\n",
    "\n",
    "parser.add_argument('--load', default=None,\n",
    "                        help='path of the checkpoint file')\n",
    "## Data Augmentation\n",
    "parser.add_argument('--feat_aug', '-faug', default='norm', type=str,\n",
    "                    help='If use feature level augmentation.')\n",
    "parser.add_argument('--task_aug', '-taug', default=[], nargs='+', type=str,\n",
    "                    help='If use task level data augmentation.')\n",
    "parser.add_argument('--support_aug', '-saug', default=None, type=str,\n",
    "                    help='If use support level data augmentation.')\n",
    "parser.add_argument('--shot_aug', '-shotaug', default=[], nargs='+', type=str,\n",
    "                    help='If use shot level data augmentation.')\n",
    "parser.add_argument('--query_aug', '-qaug', default=[], nargs='+', type=str,\n",
    "                    help='If use query level data augmentation.')\n",
    "parser.add_argument('--t_p', '-tp', default=1, type=float,\n",
    "                    help='The possibility of sampling categories or images with rot90.')\n",
    "parser.add_argument('--s_p', '-sp', default=1, type=float,\n",
    "                    help='The possibility of using support level data augmentation')\n",
    "parser.add_argument('--s_du', '-sdu', default=1, type=int,\n",
    "                    help='number of support examples augmented by shot')\n",
    "parser.add_argument('--q_p', '-qp', default=[], nargs='+', type=float,\n",
    "                    help='The possibility of using query level data augmentation')\n",
    "parser.add_argument('--rot_degree', default=30, type=int,\n",
    "                    help='Degree for random rotation when using rotation in support or query level augmentation.')\n",
    "# New Parser args for instaaug\n",
    "parser.add_argument('--Li_config_path', default=None, type=str,\n",
    "                    help=\"Config path for the learnable invariance, assuming it is included in one of the augmentations\")\n",
    "parser.add_argument('--no_random_cropping', action=\"store_true\",\n",
    "                    help=\"whether to use random cropping as a default transform\")\n",
    "parser.add_argument('--no_color_jitter', action=\"store_true\",\n",
    "                    help=\"whether to use color_jitter as a default transform\")\n",
    "parser.add_argument('--wandb', default=False, type=bool,\n",
    "                    help=\"whether to use wandb\")\n",
    "parser.add_argument('--min_entropy', default=-1, type=float,\n",
    "                    help='Minimum entropy to use in case of instance based augmentation')\n",
    "parser.add_argument('--max_entropy', default=-1, type=float,\n",
    "                    help='Maximum entropy to use in case of instance based augmentation')\n",
    "\n",
    "opt = parser.parse_args('--gpu 0 --train-shot 1 --val-shot 1 \\\n",
    "   --head ProtoNet --network ResNet --dataset CIFAR_FS --query_aug random_erase --q_p 1. \\\n",
    "   --no_color_jitter --no_random_cropping'.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-FS dataset - phase train\n",
      "CIFAR_FS(phase=train, augment=norm)\n",
      "Loading CIFAR-FS dataset - phase val\n",
      "CIFAR_FS(phase=val, augment=null)\n"
     ]
    }
   ],
   "source": [
    "if opt.wandb:\n",
    "    wandb.init(project=\"330proj\", entity=\"erictang000\")\n",
    "\n",
    "trainset = get_datasets(opt.dataset, 'train', opt)\n",
    "valset = get_datasets(opt.dataset, 'val', opt)\n",
    "\n",
    "epoch_s = opt.episodes_per_batch * opt.num_per_batch\n",
    "\n",
    "dloader_train = FewShotDataloader(trainset, kway=opt.sample_way, kshot=opt.train_shot, kquery=opt.train_query,\n",
    "                                batch_size=opt.episodes_per_batch, num_workers=0, epoch_size=epoch_s, shuffle=True)\n",
    "dloader_val = FewShotDataloader(valset, kway=opt.test_way, kshot=opt.val_shot, kquery=opt.val_query,\n",
    "                                batch_size=1, num_workers=1, epoch_size=opt.val_episode, shuffle=False, fixed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu: 0\n",
      "{'num_epoch': 40, 'save_epoch': 10, 'train_shot': 1, 'val_shot': 1, 'train_query': 6, 'val_episode': 2000, 'val_query': 15, 'train_way': 5, 'sample_way': 5, 'test_way': 5, 'save_path': './experiments/exp_1', 'gpu': '0', 'network': 'ResNet', 'head': 'ProtoNet', 'dataset': 'CIFAR_FS', 'episodes_per_batch': 8, 'num_per_batch': 1000, 'eps': 0.0, 'load': None, 'feat_aug': 'norm', 'task_aug': [], 'support_aug': None, 'shot_aug': [], 'query_aug': ['random_erase'], 't_p': 1, 's_p': 1, 's_du': 1, 'q_p': [1.0], 'rot_degree': 30, 'Li_config_path': None, 'no_random_cropping': True, 'no_color_jitter': True, 'wandb': False, 'min_entropy': -1, 'max_entropy': -1}\n"
     ]
    }
   ],
   "source": [
    "set_gpu(opt.gpu)\n",
    "check_dir('./experiments/')\n",
    "check_dir(opt.save_path)\n",
    "\n",
    "log_file_path = os.path.join(opt.save_path, \"train_log.txt\")\n",
    "log(log_file_path, str(vars(opt)))\n",
    "\n",
    "# Check whether or not to initialize instance augmentation module:\n",
    "Li_configs={'li_flag': False}\n",
    "if (opt.support_aug and \"instance\" in opt.support_aug) or (opt.query_aug and \"instance\" in opt.query_aug) or (opt.shot_aug and \"instance\" in opt.shot_aug):\n",
    "    # initialize instance augmentation module\n",
    "\n",
    "    if opt.Li_config_path:\n",
    "        Li_configs=yaml.safe_load(open(opt.Li_config_path,'r'))\n",
    "    else:\n",
    "        print(\"instance module specified, but config not provided\")\n",
    "        exit(0)\n",
    "    if opt.min_entropy != -1:\n",
    "        Li_configs[\"entropy_min_thresholds\"] = [opt.min_entropy]\n",
    "    if opt.max_entropy != -1:\n",
    "        Li_configs[\"entropy_max_thresholds\"] = [opt.max_entropy]\n",
    "    instance_aug_module = learnable_invariance(Li_configs).to(DEVICE)\n",
    "    optimizer_Li=torch.optim.SGD(instance_aug_module.parameters(), lr=Li_configs['lr'])\n",
    "    start_entropy = None\n",
    "\n",
    "if (opt.support_aug and \"random_crop\" in opt.support_aug) or (opt.query_aug and \"random_crop\" in opt.query_aug) or (opt.shot_aug and \"random_crop\" in opt.shot_aug):\n",
    "    random_crop = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(32, scale=[0.2, 1]),\n",
    "    ])\n",
    "if (opt.support_aug and \"random_erase\" in opt.support_aug) or (opt.query_aug and \"random_erase\" in opt.query_aug) or (opt.shot_aug and \"random_erase\" in opt.shot_aug):\n",
    "    random_erase = transforms.Compose([\n",
    "        transforms.RandomErasing(value=\"random\"),\n",
    "    ])\n",
    "\n",
    "    \n",
    "# Choosing whether to use mix classifier for base-learner\n",
    "if opt.support_aug and \"mix\" in opt.support_aug:\n",
    "    (embedding_net, cls_head, cls_head_mixup) = get_model(opt)\n",
    "    embedding_net.to(DEVICE)\n",
    "    cls_head.to(DEVICE)\n",
    "    cls_head_mixup.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.SGD([{'params': embedding_net.parameters()}, \n",
    "                                    {'params': cls_head_mixup.parameters()}], lr=0.1, momentum=0.9, \\\n",
    "                                        weight_decay=5e-4, nesterov=True)\n",
    "else:\n",
    "    (embedding_net, cls_head) = get_model(opt)\n",
    "    embedding_net = embedding_net.to(DEVICE)\n",
    "    cls_head = cls_head.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.SGD([{'params': embedding_net.parameters()}, \n",
    "                                    {'params': cls_head.parameters()}], lr=0.1, momentum=0.9, \\\n",
    "                                        weight_decay=5e-4, nesterov=True)\n",
    "\n",
    "lambda_epoch = lambda e: 1.0 if e < 20 else (0.06 if e < 40 else 0.012 if e < 50 else (0.0024 if e < 60 else (0.001)))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch, last_epoch=-1)\n",
    "\n",
    "max_val_acc = 0.0\n",
    "\n",
    "timer = Timer()\n",
    "x_entropy = torch.nn.CrossEntropyLoss()\n",
    "x_entropy_Li = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLearning Rate: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "/data/erictang000/miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3334: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/data/erictang000/miniconda3/lib/python3.8/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tBatch: [100/1000]\tLoss: 1.3287\tAccuracy: 40.18 % (45.42 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tBatch: [200/1000]\tLoss: 1.1946\tAccuracy: 43.23 % (56.67 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/erictang000/meta_instance_aug/train_aug.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brainbowquartz/data/erictang000/meta_instance_aug/train_aug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m train_losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brainbowquartz/data/erictang000/meta_instance_aug/train_aug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m train_LI_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brainbowquartz/data/erictang000/meta_instance_aug/train_aug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dloader_train(epoch), \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brainbowquartz/data/erictang000/meta_instance_aug/train_aug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrandom_crop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m opt\u001b[39m.\u001b[39mshot_aug:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brainbowquartz/data/erictang000/meta_instance_aug/train_aug.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         data_support, labels_support, dc_s, data_query, labels_query, dc_q \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/erictang000/meta_instance_aug/metadatas/utils.py:74\u001b[0m, in \u001b[0;36mListDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     73\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCustomRange index out of range\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlist[idx])\n",
      "File \u001b[0;32m/data/erictang000/meta_instance_aug/metadatas/utils.py:205\u001b[0m, in \u001b[0;36mFewShotDataloader.get_iterator.<locals>.load_function\u001b[0;34m(iter_idx)\u001b[0m\n\u001b[1;32m    203\u001b[0m categories \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampleCategories(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkway)\n\u001b[1;32m    204\u001b[0m shot_e, query_e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_examples_for_categories(categories, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkshot, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkquery)\n\u001b[0;32m--> 205\u001b[0m Xt, Yt, Ot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mcreateExamplesTensorData(query_e)\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shot_e) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    207\u001b[0m     Xe, Ye, Oe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mcreateExamplesTensorData(shot_e)\n",
      "File \u001b[0;32m/data/erictang000/meta_instance_aug/metadatas/utils.py:303\u001b[0m, in \u001b[0;36mProtoData.createExamplesTensorData\u001b[0;34m(self, examples, method)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreateExamplesTensorData\u001b[39m(\u001b[39mself\u001b[39m, examples, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msupport\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    281\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m    Creates the examples image and label tensor data.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39m            of each example.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\n\u001b[0;32m--> 303\u001b[0m         [\u001b[39mself\u001b[39m[img_idx][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m img_idx, _, _ \u001b[39min\u001b[39;00m examples], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    305\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([label \u001b[39mfor\u001b[39;00m _, label, _ \u001b[39min\u001b[39;00m examples])\n\u001b[1;32m    306\u001b[0m     dc_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([label \u001b[39mfor\u001b[39;00m _, _, label \u001b[39min\u001b[39;00m examples])\n",
      "File \u001b[0;32m/data/erictang000/meta_instance_aug/metadatas/utils.py:303\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreateExamplesTensorData\u001b[39m(\u001b[39mself\u001b[39m, examples, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msupport\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    281\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m    Creates the examples image and label tensor data.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39m            of each example.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\n\u001b[0;32m--> 303\u001b[0m         [\u001b[39mself\u001b[39;49m[img_idx][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m img_idx, _, _ \u001b[39min\u001b[39;00m examples], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    305\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([label \u001b[39mfor\u001b[39;00m _, label, _ \u001b[39min\u001b[39;00m examples])\n\u001b[1;32m    306\u001b[0m     dc_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor([label \u001b[39mfor\u001b[39;00m _, _, label \u001b[39min\u001b[39;00m examples])\n",
      "File \u001b[0;32m/data/erictang000/meta_instance_aug/metadatas/CIFAR_FS.py:78\u001b[0m, in \u001b[0;36mCIFAR_FS.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     76\u001b[0m original_img \u001b[39m=\u001b[39m img\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m     79\u001b[0m \u001b[39mreturn\u001b[39;00m img, label\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    163\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[0;32m--> 164\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "File \u001b[0;32m/data/erictang000/miniconda3/lib/python3.8/site-packages/PIL/Image.py:510\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyaccess \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exif \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    512\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    513\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mImage categories are deprecated and will be removed in Pillow 10 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(2023-07-01). Use is_animated instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    515\u001b[0m             \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    516\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    517\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(1, opt.num_epoch + 1):\n",
    "    # Train on the training split\n",
    "    \n",
    "    # Fetch the current epoch's learning rate\n",
    "    epoch_learning_rate = 0.1\n",
    "    for param_group in optimizer.param_groups:\n",
    "        epoch_learning_rate = param_group['lr']\n",
    "        \n",
    "    log(log_file_path, 'Train Epoch: {}\\tLearning Rate: {:.4f}'.format(\n",
    "                        epoch, epoch_learning_rate))\n",
    "    \n",
    "    _, _ = [x.train() for x in (embedding_net, cls_head)]\n",
    "    \n",
    "    train_accuracies = []\n",
    "    train_s_accuracies = []\n",
    "    train_losses = []\n",
    "    train_LI_losses = []\n",
    "\n",
    "    for i, batch in enumerate(dloader_train(epoch), 1):\n",
    "        if \"random_crop\" in opt.shot_aug:\n",
    "            data_support, labels_support, dc_s, data_query, labels_query, dc_q = [x.to(DEVICE) for x in batch]\n",
    "        elif (opt.support_aug and \"instance\" in opt.support_aug) or (opt.query_aug and \"instance\" in opt.query_aug) or (opt.shot_aug and \"instance\" in opt.shot_aug):\n",
    "            data_support, labels_support, dc_s, data_query, labels_query, dc_q = [x.to(DEVICE) for x in batch]\n",
    "        else:\n",
    "            data_support, labels_support, dc_s, data_query, labels_query, dc_q = [x for x in batch]\n",
    "\n",
    "        train_n_support = opt.train_way * opt.train_shot \n",
    "        train_n_query = opt.train_way * opt.train_query \n",
    "        rs, rq = 0., 0.\n",
    "\n",
    "        ## data augmentation for shots (increasing num of shots for support)\n",
    "        for shot_method in opt.shot_aug:\n",
    "            if shot_method == \"instance\":\n",
    "                data_support, labels_support, train_n_support, _, _ = shot_aug(data_support, labels_support, train_n_support, shot_method, opt, instance_aug_module)\n",
    "            elif shot_method == \"random_crop\":\n",
    "                data_support, labels_support, train_n_support = shot_aug(data_support, labels_support, train_n_support, shot_method, opt, random_crop=random_crop)\n",
    "            elif shot_method == \"random_erase\":\n",
    "                data_support, labels_support, train_n_support = shot_aug(data_support, labels_support, train_n_support, shot_method, opt, random_erase=random_erase)\n",
    "            else:\n",
    "                data_support, labels_support, train_n_support = shot_aug(data_support, labels_support, train_n_support, shot_method, opt)\n",
    "        ## data augmentation for support data\n",
    "        if opt.support_aug and \"instance\" in opt.support_aug:\n",
    "            e, b = data_support.shape[0], data_support.shape[1]\n",
    "            data_support = data_support.view((-1, data_support.shape[2], data_support.shape[3], data_support.shape[4]))\n",
    "            data_support, _, _, _ = apply_instance_aug(instance_aug_module, data_support)\n",
    "            data_support = data_support.view((e, b, data_support.shape[1], data_support.shape[2], data_support.shape[3]))\n",
    "        elif opt.support_aug and \"random_crop\" in opt.support_aug:\n",
    "            e, b = data_support.shape[0], data_support.shape[1]\n",
    "            data_support = data_support.view((-1, data_support.shape[2], data_support.shape[3], data_support.shape[4]))\n",
    "            data_support = random_crop(data_support)\n",
    "            data_support = data_support.view((e, b, data_support.shape[1], data_support.shape[2], data_support.shape[3]))\n",
    "        elif opt.support_aug and \"random_erase\" in opt.support_aug:\n",
    "            e, b = data_support.shape[0], data_support.shape[1]\n",
    "            data_support = data_support.view((-1, data_support.shape[2], data_support.shape[3], data_support.shape[4]))\n",
    "            data_support = random_erase(data_support)\n",
    "            data_support = data_support.view((e, b, data_support.shape[1], data_support.shape[2], data_support.shape[3]))\n",
    "        elif opt.support_aug and \"mix\" in opt.support_aug:\n",
    "            data_support, label_support_a, label_support_b, lss, rs = data_aug_mix(data_support, labels_support, opt.s_p, opt.support_aug, opt)\n",
    "            label_support_a, label_support_b = label_support_a.to(DEVICE), label_support_b.to(DEVICE)\n",
    "        elif opt.support_aug:\n",
    "            data_support, labels_support = data_aug(data_support, labels_support, opt.s_p, opt.support_aug, opt)\n",
    "\n",
    "        ## data augmentation for query data\n",
    "        for mi, query_method in enumerate(opt.query_aug):\n",
    "            ## instance augmentation\n",
    "            if \"instance\" in query_method:\n",
    "                e, b = data_query.shape[0], data_query.shape[1]\n",
    "                data_query = data_query.view((-1, data_query.shape[2], data_query.shape[3], data_query.shape[4]))\n",
    "                # if i % 20 == 0:\n",
    "                #     plt.imshow(np.transpose(data_query[0].detach().cpu().numpy(), (1, 2, 0)))\n",
    "                #     plt.imsave(\"before.png\", np.transpose(data_query[0].detach().cpu().numpy(), (1, 2, 0)).clip(0,1))\n",
    "                data_query, logprobs, entropy_every, _, num_erased = apply_instance_aug(instance_aug_module, data_query)\n",
    "                # if i % 20 == 0:\n",
    "                #     print(num_erased)\n",
    "                #     plt.imshow(np.transpose(data_query[0].detach().cpu().numpy(), (1, 2, 0)))\n",
    "                #     plt.imsave(\"after.png\", np.transpose(data_query[0].detach().cpu().numpy(), (1, 2, 0)).clip(0,1))\n",
    "                data_query = data_query.view((e, b, data_query.shape[1], data_query.shape[2], data_query.shape[3]))\n",
    "            elif \"random_crop\" in query_method:\n",
    "                e, b = data_query.shape[0], data_query.shape[1]\n",
    "                data_query = data_query.view((-1, data_query.shape[2], data_query.shape[3], data_query.shape[4]))\n",
    "                data_query = random_crop(data_query)\n",
    "                data_query = data_query.view((e, b, data_query.shape[1], data_query.shape[2], data_query.shape[3]))\n",
    "            elif \"random_erase\" in query_method:\n",
    "                e, b = data_query.shape[0], data_query.shape[1]\n",
    "                data_query = data_query.view((-1, data_query.shape[2], data_query.shape[3], data_query.shape[4]))\n",
    "                data_query = random_erase(data_query)\n",
    "                data_query = data_query.view((e, b, data_query.shape[1], data_query.shape[2], data_query.shape[3]))\n",
    "            elif \"mix\" in query_method:\n",
    "                data_query, label_query_a, label_query_b, lqs, rq = data_aug_mix(data_query, labels_query, opt.q_p[mi], query_method, opt)\n",
    "                mixp = opt.q_p[mi]\n",
    "                label_query_a, label_query_b = label_query_a.to(DEVICE), label_query_b.to(DEVICE)\n",
    "            else:\n",
    "                data_query, labels_query = data_aug(data_query, labels_query, opt.q_p[mi], query_method, opt)\n",
    "\n",
    "        ## related with the augmentation combine, where the sample ways are larger than train ways\n",
    "        if data_support.shape[1] != train_n_support:\n",
    "            data_support = data_support[labels_support < opt.train_way]\n",
    "            labels_support = labels_support[labels_support < opt.train_way]\n",
    "        if data_query.shape[1] != train_n_query:\n",
    "            data_query = data_query[labels_query < opt.train_way]\n",
    "            labels_query = labels_query[labels_query < opt.train_way]\n",
    "            \n",
    "        data_support, labels_support, data_query, labels_query = data_support.to(DEVICE), labels_support.to(DEVICE), data_query.to(DEVICE), labels_query.to(DEVICE)\n",
    "        dc_s, dc_q = dc_s.to(DEVICE), dc_q.to(DEVICE)\n",
    "\n",
    "        ## get embedding\n",
    "        prob = random.random()\n",
    "        if opt.support_aug and opt.support_aug == 'feature_mixup' and prob < opt.s_p:\n",
    "            emb_support, label_support_a, label_support_b, lss = embedding_net(data_support.reshape([-1] + list(data_support.shape[-3:])), target=labels_support, mixup_hidden=True, opt=opt)\n",
    "        else:\n",
    "            emb_support = embedding_net(data_support.reshape([-1] + list(data_support.shape[-3:])))\n",
    "        emb_support = emb_support.reshape(opt.episodes_per_batch, train_n_support, -1)\n",
    "        \n",
    "        if opt.query_aug and opt.query_aug == 'feature_mixup' and prob < opt.q_p :\n",
    "            emb_query, label_query_a, label_query_b, lqs = embedding_net(data_query.reshape([-1] + list(data_query.shape[-3:])), target=labels_query, mixup_hidden=True, opt=opt)\n",
    "        else:\n",
    "            emb_query = embedding_net(data_query.reshape([-1] + list(data_query.shape[-3:])))\n",
    "        emb_query = emb_query.reshape(opt.episodes_per_batch, train_n_query, -1)\n",
    "        \n",
    "        ## get logits for query embedding\n",
    "        if opt.support_aug and \"mix\" in opt.support_aug and rs < opt.s_p:\n",
    "            logit_query = cls_head_mixup(emb_query, emb_support, label_support_a, label_support_b, lss, opt.train_way, opt.train_shot)\n",
    "        else:\n",
    "            logit_query = cls_head(emb_query, emb_support, labels_support, opt.train_way, opt.train_shot * opt.s_du)\n",
    "            labels_support = labels_support.view(-1)\n",
    "\n",
    "        ## get loss for the outer loop\n",
    "        is_query_mix = any('mix' in m for m in opt.query_aug)\n",
    "        if is_query_mix and rq < mixp:\n",
    "            loss = mixup_criterion(x_entropy, logit_query, label_query_a, label_query_b, lqs)\n",
    "            acc = count_accuracy_mixup(logit_query, label_query_a, label_query_b, lqs)\n",
    "        else:\n",
    "            logit_query = logit_query.view(-1, opt.train_way)\n",
    "            labels_query = labels_query.view(-1)\n",
    "            loss = x_entropy(logit_query, labels_query)\n",
    "            acc = count_accuracy(logit_query, labels_query)\n",
    "\n",
    "\n",
    "        ## get accuracies\n",
    "        train_accuracies.append(acc.item())\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if (i % 100 == 0):\n",
    "            train_acc_avg = np.mean(np.array(train_accuracies))\n",
    "            train_li_loss_avg = np.mean(np.array(train_LI_losses))\n",
    "            if opt.wandb:\n",
    "                if Li_configs['li_flag']:\n",
    "                    wandb.log({\"train loss\": loss.item(), \"train_acc_avg\": train_acc_avg, \"train_acc\": acc, \"li_loss_avg\": train_li_loss_avg})\n",
    "                else:\n",
    "                    wandb.log({\"train loss\": loss.item(), \"train_acc_avg\": train_acc_avg, \"train_acc\": acc})\n",
    "            log(log_file_path, 'Train Epoch: {}\\tBatch: [{}/{}]\\tLoss: {:.4f}\\tAccuracy: {:.2f} % ({:.2f} %)'.format(\n",
    "                        epoch, i, len(dloader_train), loss.item(), train_acc_avg, acc))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    \n",
    "        if Li_configs['li_flag']:\n",
    "            # assume we only ever do instaaug on its own\n",
    "            embedding_net.eval()\n",
    "            cls_head.eval()\n",
    "            optimizer_Li.zero_grad() #Don't forget zero_grad\n",
    "\n",
    "            emb_support = embedding_net(data_support.reshape([-1] + list(data_support.shape[-3:])))\n",
    "            emb_support = emb_support.reshape(opt.episodes_per_batch, train_n_support, -1)\n",
    "            emb_query = embedding_net(data_query.reshape([-1] + list(data_query.shape[-3:])))\n",
    "            emb_query = emb_query.reshape(opt.episodes_per_batch, train_n_query, -1)\n",
    "            logit_query = cls_head(emb_query, emb_support, labels_support, opt.train_way, opt.train_shot * opt.s_du)\n",
    "            labels_support = labels_support.view(-1)\n",
    "            logit_query = logit_query.view(-1, opt.train_way)\n",
    "            labels_query = labels_query.view(-1)\n",
    "\n",
    "\n",
    "            loss_predictor = x_entropy_Li(logit_query, labels_query)\n",
    "            loss_Li_pre=(loss_predictor.detach()*logprobs.to(loss_predictor.device)).mean()+loss_predictor.mean() #!The last half\n",
    "            if (i % 100 == 0):\n",
    "                print(\"first part of loss\" + str(loss_Li_pre))\n",
    "            loss_Li_pre -= (entropy_every.mean(dim=0) * torch.tensor(Li_configs['entropy_weights']).type(entropy_every.type())).sum()\n",
    "            if (i % 100 == 0):\n",
    "                print(\"total loss\" + str(loss_Li_pre))\n",
    "            loss_Li = loss_Li_pre\n",
    "            if 'warmup_period' in Li_configs and Li_configs['warmup_period']> 0 and epoch < Li_configs['warmup_period']:\n",
    "                optimizer_Li.param_groups[0]['lr'] = Li_configs['lr']/Li_configs['warmup_period']*epoch\n",
    "            else:\n",
    "                optimizer_Li.param_groups[0]['lr'] = Li_configs['lr']\n",
    "\n",
    "            for j in range(len(Li_configs['entropy_min_thresholds'])):\n",
    "                entropy_step = instance_aug_module.schedulers[j].step(entropy_every.mean(0))\n",
    "                Li_configs['entropy_weights'][j] *= entropy_step\n",
    "                # if entropy_step != 1:\n",
    "                #     print('entropy_weight: {}, {}'.format(j, Li_configs['entropy_weights'][j]))\n",
    "            if (i % 100 == 0):\n",
    "                print(\"entropy_weights: {}\".format(Li_configs['entropy_weights']))\n",
    "                print(\"mean entropy: {}\".format(entropy_every.mean()))\n",
    "\n",
    "\n",
    "            loss_Li.backward()\n",
    "            optimizer_Li.step()\n",
    "            \n",
    "            train_LI_losses.append(loss_Li.detach().cpu().item())\n",
    "            \n",
    "            embedding_net.train()\n",
    "            cls_head.train()\n",
    "\n",
    "    # Evaluate on the validation split\n",
    "    if opt.support_aug and \"mix\" in opt.support_aug:\n",
    "        cls_head.load_state_dict(cls_head_mixup.state_dict())\n",
    "    _, _ = [x.eval() for x in (embedding_net, cls_head)]\n",
    "\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for i, batch in enumerate(dloader_val(epoch), 1):\n",
    "        data_support, labels_support, _, data_query, labels_query, _ = [x.to(DEVICE) for x in batch]\n",
    "\n",
    "        test_n_support = opt.test_way * opt.val_shot\n",
    "        test_n_query = opt.test_way * opt.val_query\n",
    "\n",
    "        for method in opt.shot_aug:\n",
    "            if method == \"instance\":\n",
    "                data_support, labels_support, test_n_support, _, _ = shot_aug(data_support, labels_support, test_n_support, method, opt, instance_aug_module)\n",
    "            elif method == \"random_crop\":\n",
    "                data_support, labels_support, test_n_support = shot_aug(data_support, labels_support, test_n_support, method, opt, random_crop=random_crop)\n",
    "            else:\n",
    "                data_support, labels_support, test_n_support = shot_aug(data_support, labels_support, test_n_support, method, opt)\n",
    "\n",
    "        emb_support = embedding_net(data_support.reshape([-1] + list(data_support.shape[-3:])))\n",
    "        emb_support = emb_support.reshape(1, test_n_support, -1)\n",
    "        emb_query = embedding_net(data_query.reshape([-1] + list(data_query.shape[-3:])))\n",
    "        emb_query = emb_query.reshape(1, test_n_query, -1)\n",
    "\n",
    "        logit_query = cls_head(emb_query, emb_support, labels_support, opt.test_way, opt.val_shot * opt.s_du)[0]\n",
    "\n",
    "        loss = x_entropy(logit_query.reshape(-1, opt.test_way), labels_query.reshape(-1))\n",
    "        acc = count_accuracy(logit_query.reshape(-1, opt.test_way), labels_query.reshape(-1))\n",
    "\n",
    "        val_accuracies.append(acc.item())\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "    val_acc_avg = np.mean(np.array(val_accuracies))\n",
    "    val_acc_ci95 = 1.96 * np.std(np.array(val_accuracies)) / np.sqrt(opt.val_episode)\n",
    "\n",
    "    val_loss_avg = np.mean(np.array(val_losses))\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    if val_acc_avg > max_val_acc:\n",
    "        max_val_acc = val_acc_avg\n",
    "        torch.save({'embedding': embedding_net.state_dict(), 'head': cls_head.state_dict()},\\\n",
    "                    os.path.join(opt.save_path, 'best_model.pth'))\n",
    "        if opt.wandb:\n",
    "            wandb.log({\"val loss\": val_loss_avg, \"val_acc_avg\": val_acc_avg,\"val_acc\": val_acc_ci95})\n",
    "        log(log_file_path, 'Validation Epoch: {}\\t\\t\\tLoss: {:.4f}\\tAccuracy: {:.2f} ± {:.2f} % (Best)'\\\n",
    "                .format(epoch, val_loss_avg, val_acc_avg, val_acc_ci95))\n",
    "    else:\n",
    "        if opt.wandb:\n",
    "            wandb.log({\"val loss\": val_loss_avg, \"val_acc_avg\": val_acc_avg,\"val_acc\": val_acc_ci95})\n",
    "        log(log_file_path, 'Validation Epoch: {}\\t\\t\\tLoss: {:.4f}\\tAccuracy: {:.2f} ± {:.2f} %'\\\n",
    "                .format(epoch, val_loss_avg, val_acc_avg, val_acc_ci95))\n",
    "\n",
    "    if Li_configs['li_flag']:\n",
    "        torch.save({'embedding': embedding_net.state_dict(), 'head': cls_head.state_dict(), 'li': instance_aug_module.state_dict()}\\\n",
    "                , os.path.join(opt.save_path, 'last_epoch.pth'))\n",
    "    else:\n",
    "        torch.save({'embedding': embedding_net.state_dict(), 'head': cls_head.state_dict()}\\\n",
    "                , os.path.join(opt.save_path, 'last_epoch.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'crop',\n",
       " 'dataset': 'cifar100',\n",
       " 'transform': ['crop'],\n",
       " 'random_aug': False,\n",
       " 'global_aug': False,\n",
       " 'li_flag': True,\n",
       " 'load_li': False,\n",
       " 'train_copies': 1,\n",
       " 'test_time_aug': False,\n",
       " 'test_copies': 10,\n",
       " 'lr': 1e-05,\n",
       " 'warmup_period': 5,\n",
       " 'crop_layer': [2, -1],\n",
       " 'crop_layer_bias': [1, 3],\n",
       " 'entropy_weights': [1820.0876300004268],\n",
       " 'entropy_max_thresholds': [3.5],\n",
       " 'entropy_min_thresholds': [3.0],\n",
       " 'entropy_increase_period': 40,\n",
       " 'scheduler_sleep_epoch': 0,\n",
       " 'ConvFeature': True,\n",
       " 'contrastive_train_aug': False,\n",
       " 'contrastive_train_copies': 2,\n",
       " 'contrastive_train_output_max': 2,\n",
       " 'contrastive_test_aug': False,\n",
       " 'contrastive_test_copies': 2,\n",
       " 'contrastive_test_output_max': 2,\n",
       " 'max_black_ratio': 0.2,\n",
       " 'crop_only_for_tpu': True,\n",
       " 'zoom_min': 0.3,\n",
       " 'zoom_max': 1.1,\n",
       " 'zoom_step': 50,\n",
       " 'translation_min': -1,\n",
       " 'translation_max': 1,\n",
       " 'translation_step': 50,\n",
       " 'input_size': 32}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Li_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'crop',\n",
       " 'dataset': 'cifar100',\n",
       " 'transform': ['crop'],\n",
       " 'random_aug': False,\n",
       " 'global_aug': False,\n",
       " 'li_flag': True,\n",
       " 'load_li': False,\n",
       " 'train_copies': 1,\n",
       " 'test_time_aug': False,\n",
       " 'test_copies': 10,\n",
       " 'lr': 1e-05,\n",
       " 'warmup_period': 5,\n",
       " 'crop_layer': [2, -1],\n",
       " 'crop_layer_bias': [1, 3],\n",
       " 'entropy_weights': [0.2],\n",
       " 'entropy_max_thresholds': [3.5],\n",
       " 'entropy_min_thresholds': [3.0],\n",
       " 'entropy_increase_period': 40,\n",
       " 'scheduler_sleep_epoch': 0,\n",
       " 'ConvFeature': True,\n",
       " 'contrastive_train_aug': False,\n",
       " 'contrastive_train_copies': 2,\n",
       " 'contrastive_train_output_max': 2,\n",
       " 'contrastive_test_aug': False,\n",
       " 'contrastive_test_copies': 2,\n",
       " 'contrastive_test_output_max': 2,\n",
       " 'max_black_ratio': 0.2,\n",
       " 'crop_only_for_tpu': True,\n",
       " 'zoom_min': 0.3,\n",
       " 'zoom_max': 1.1,\n",
       " 'zoom_step': 50,\n",
       " 'translation_min': -1,\n",
       " 'translation_max': 1,\n",
       " 'translation_step': 50,\n",
       " 'input_size': 32}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Li_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1ab61a329566cc13b7878ae693f1437cd38df5189cbdd89ad8e228bdbc8c6fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
