{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from metadatas import *\n",
    "from augmentations import *\n",
    "\n",
    "from models.classification_heads import ClassificationHead, R2D2Head\n",
    "from models.classification_heads import ClassificationHead_Mixup, R2D2Head_Mixup\n",
    "from models.R2D2_embedding import R2D2Embedding\n",
    "from models.R2D2_embedding_mixup import R2D2Embedding_mixup\n",
    "from models.protonet_embedding import ProtoNetEmbedding\n",
    "from models.ResNet12_embedding import resnet12 \n",
    "from models.ResNet12_embedding_mixup import resnet12_mixup \n",
    "from InstaAug_module import learnable_invariance\n",
    "\n",
    "from utils import set_gpu, Timer, count_accuracy, count_accuracy_mixup, check_dir, log\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import time\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "\n",
    "#np.seterr(all='raise')\n",
    "\n",
    "def mixup_data(x, y, lam, use_cuda=False):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).to(DEVICE)\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    loss = 0.\n",
    "    for i in range(len(pred)):\n",
    "        loss += lam[i] * criterion(pred[i], y_a[i]) + (1 - lam[i]) * criterion(pred[i], y_b[i])\n",
    "\n",
    "    return loss/len(pred)\n",
    "\n",
    "\n",
    "def one_hot(indices, depth):\n",
    "    \"\"\"\n",
    "    Returns a one-hot tensor.\n",
    "    This is a PyTorch equivalent of Tensorflow's tf.one_hot.\n",
    "        \n",
    "    Parameters:\n",
    "      indices:  a (n_batch, m) Tensor or (m) Tensor.\n",
    "      depth: a scalar. Represents the depth of the one hot dimension.\n",
    "    Returns: a (n_batch, m, depth) Tensor or (m, depth) Tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).to(DEVICE)\n",
    "    index = indices.view(indices.size()+torch.Size([1]))\n",
    "    encoded_indicies = encoded_indicies.scatter_(1,index,1)\n",
    "    \n",
    "    return encoded_indicies\n",
    "\n",
    "def get_model(options):\n",
    "    # Choose the embedding network\n",
    "    if options.network == 'ProtoNet':\n",
    "        network = ProtoNetEmbedding().to(DEVICE)\n",
    "    elif options.network == 'R2D2':\n",
    "        network = R2D2Embedding().to(DEVICE)\n",
    "    elif options.network == 'R2D2_mixup':\n",
    "        network = R2D2Embedding_mixup().to(DEVICE)\n",
    "    elif options.network == 'ResNet_mixup':\n",
    "        network = resnet12_mixup(avg_pool=False, drop_rate=0.1, dropblock_size=2).to(DEVICE)\n",
    "    elif options.network == 'ResNet':\n",
    "        if options.dataset == 'miniImageNet' or options.dataset == 'tieredImageNet':\n",
    "            network = resnet12(avg_pool=False, drop_rate=0.1, dropblock_size=4).to(DEVICE)\n",
    "            network = torch.nn.DataParallel(network)\n",
    "        else:\n",
    "            network = resnet12(avg_pool=False, drop_rate=0.1, dropblock_size=2).to(DEVICE)\n",
    "            network = torch.nn.DataParallel(network)\n",
    "    else:\n",
    "        print (\"Cannot recognize the network type\")\n",
    "        assert(False)\n",
    "        \n",
    "    # Choose the classification head\n",
    "    if options.head == 'ProtoNet':\n",
    "        cls_head = ClassificationHead(base_learner='ProtoNet').to(DEVICE)\n",
    "    elif options.head == 'Ridge':\n",
    "        cls_head = ClassificationHead(base_learner='Ridge').to(DEVICE)\n",
    "    elif options.head == 'R2D2':\n",
    "        cls_head = R2D2Head().to(DEVICE) \n",
    "    elif options.head == 'SVM':\n",
    "        cls_head = ClassificationHead(base_learner='SVM-CS').to(DEVICE)\n",
    "    else:\n",
    "        print (\"Cannot recognize the dataset type\")\n",
    "        assert(False)\n",
    "        \n",
    "    if options.support_aug and 'mix' in options.support_aug:\n",
    "        if options.head == 'R2D2':\n",
    "            cls_head_mixup = R2D2Head_Mixup().to(DEVICE)\n",
    "        elif options.head == 'SVM':\n",
    "            cls_head_mixup = ClassificationHead_Mixup(base_learner='SVM-CS').to(DEVICE)\n",
    "        else:\n",
    "            print(\"Cannot recognize the dataset type\")\n",
    "\n",
    "        return (network, cls_head, cls_head_mixup)\n",
    "        \n",
    "    else:\n",
    "        return (network, cls_head)\n",
    "\n",
    "\n",
    "def get_datasets(name, phase, args):\n",
    "    random_cropping = False if args.no_random_cropping else True\n",
    "    color_jitter = False if args.no_color_jitter else True\n",
    "    \n",
    "    if name == 'miniImageNet':\n",
    "        dataset = MiniImageNet(phase=phase, augment=args.feat_aug, rot90_p=args.t_p, random_cropping=random_cropping, color_jitter=color_jitter)  \n",
    "    elif name == 'CIFAR_FS':\n",
    "        dataset = CIFAR_FS(phase=phase, augment=args.feat_aug, rot90_p=args.t_p, random_cropping=random_cropping, color_jitter=color_jitter)\n",
    "    # elif name == 'FC100':\n",
    "    #     dataset = FC100(phase=phase, augment=args.feat_aug, rot90_p=args.t_p)\n",
    "    else:\n",
    "        print (\"Cannot recognize the dataset type\")\n",
    "        assert(False)\n",
    "    print(dataset)\n",
    "\n",
    "    if phase == 'train':\n",
    "        for ta in args.task_aug:\n",
    "            if ta == 'Rot90':\n",
    "                dataset = Rot90(dataset, p=args.t_p, batch_size_down=8e4)\n",
    "                dataset.batch_num.value += args.num_epoch * 0.\n",
    "            elif ta == 'Mix':\n",
    "                dataset = TaskAug(dataset, \"Mix\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'CutMix':\n",
    "                dataset = TaskAug(dataset, \"CutMix\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'FMix':\n",
    "                dataset = TaskAug(dataset, \"FMix\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'Combine':\n",
    "                dataset = TaskAug(dataset, \"Combine\", p=args.t_p, batch_size_down=8e4)\n",
    "            elif ta == 'DropChannel':\n",
    "                dataset = DropChannels(dataset, p=args.t_p) \n",
    "            elif ta == 'RE':\n",
    "                dataset = RE(dataset, p=args.t_p) \n",
    "            elif ta == 'Solarize':\n",
    "                dataset = Solarize(dataset, p=args.t_p) \n",
    "            else:\n",
    "                print (\"Cannot recognize the task augmentation type\")\n",
    "                continue\n",
    "            print(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def apply_instance_aug(instance_aug_module, x, dataset_source=\"CIFAR_FS\"):\n",
    "    '''\n",
    "    Params:\n",
    "        instance_aug_module: torch.Module\n",
    "        x: input data\n",
    "        dataset source: str (either TinyImageNet or CIFAR_FS)\n",
    "\n",
    "    Returns:\n",
    "        instance augmentation applied on x\n",
    "    '''\n",
    "    return instance_aug_module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num-epoch', type=int, default=40,\n",
    "                        help='number of training epochs')\n",
    "parser.add_argument('--val-shot', type=int, default=5,\n",
    "                        help='number of support examples per validation class')\n",
    "parser.add_argument('--val-episode', type=int, default=2000,\n",
    "                        help='number of episodes per validation')\n",
    "parser.add_argument('--val-query', type=int, default=15,\n",
    "                        help='number of query examples per validation class')\n",
    "parser.add_argument('--sample-way', type=int, default=5,\n",
    "                        help='number of classes sampled in one training episode (i.e. sample 10 classes for task mixup)')\n",
    "parser.add_argument('--test-way', type=int, default=5,\n",
    "                        help='number of classes in one test (or validation) episode')\n",
    "parser.add_argument('--gpu', default='0')\n",
    "parser.add_argument('--network', type=str, default='ProtoNet',\n",
    "                        help='choose which embedding network to use. ProtoNet, R2D2, ResNet')\n",
    "parser.add_argument('--head', type=str, default='ProtoNet',\n",
    "                        help='choose which classification head to use. ProtoNet, Ridge, R2D2, SVM')\n",
    "parser.add_argument('--dataset', type=str, default='miniImageNet',\n",
    "                        help='choose which classification head to use. miniImageNet, tieredImageNet, CIFAR_FS, FC100')\n",
    "parser.add_argument('--episodes-per-batch', type=int, default=8,\n",
    "                        help='number of episodes per batch')\n",
    "parser.add_argument('--num-per-batch', type=int, default=1000,\n",
    "                        help='number of epoch size per train epoch')\n",
    "parser.add_argument('--eps', type=float, default=0.0,\n",
    "                        help='epsilon of label smoothing')\n",
    "parser.add_argument('--load', default=None,\n",
    "                        help='path of the checkpoint file')\n",
    "## Data Augmentation\n",
    "parser.add_argument('--feat_aug', '-faug', default='norm', type=str,\n",
    "                    help='If use feature level augmentation.')\n",
    "parser.add_argument('--task_aug', '-taug', default=[], nargs='+', type=str,\n",
    "                    help='If use task level data augmentation.')\n",
    "parser.add_argument('--support_aug', '-saug', default=None, type=str,\n",
    "                    help='If use support level data augmentation.')\n",
    "parser.add_argument('--shot_aug', '-shotaug', default=[], nargs='+', type=str,\n",
    "                    help='If use shot level data augmentation.')\n",
    "parser.add_argument('--query_aug', '-qaug', default=[], nargs='+', type=str,\n",
    "                    help='If use query level data augmentation.')\n",
    "parser.add_argument('--t_p', '-tp', default=1, type=float,\n",
    "                    help='The possibility of sampling categories or images with rot90.')\n",
    "parser.add_argument('--s_p', '-sp', default=1, type=float,\n",
    "                    help='The possibility of using support level data augmentation')\n",
    "parser.add_argument('--s_du', '-sdu', default=1, type=int,\n",
    "                    help='number of support examples augmented by shot')\n",
    "parser.add_argument('--q_p', '-qp', default=[], nargs='+', type=float,\n",
    "                    help='The possibility of using query level data augmentation')\n",
    "parser.add_argument('--rot_degree', default=30, type=int,\n",
    "                    help='Degree for random rotation when using rotation in support or query level augmentation.')\n",
    "# New Parser args for instaaug\n",
    "parser.add_argument('--Li_config_path', default=None, type=str,\n",
    "                    help=\"Config path for the learnable invariance, assuming it is included in one of the augmentations\")\n",
    "parser.add_argument('--no_random_cropping', action=\"store_true\",\n",
    "                    help=\"whether to use random cropping as a default transform\")\n",
    "parser.add_argument('--no_color_jitter', action=\"store_true\",\n",
    "                    help=\"whether to use color_jitter as a default transform\")\n",
    "parser.add_argument('--wandb', default=False, type=bool,\n",
    "                    help=\"whether to use wandb\")\n",
    "parser.add_argument('--min_entropy', default=-1, type=float,\n",
    "                    help='Minimum entropy to use in case of instance based augmentation')\n",
    "parser.add_argument('--max_entropy', default=-1, type=float,\n",
    "                    help='Maximum entropy to use in case of instance based augmentation')\n",
    "parser.add_argument('--test_time_aug_samples', default=1, type=int,\n",
    "                    help=\"Number of samples to use for test time data augmentation\")\n",
    "\n",
    "opt = parser.parse_args('--gpu 0 --val-shot 5 \\\n",
    "   --head ProtoNet --network ResNet --dataset CIFAR_FS \\\n",
    "   --no_color_jitter --no_random_cropping --Li_config_path ./InstaAug_module/configs/config_crop_supervised_cifar.yaml \\\n",
    "   --load ./experiments/CIFAR_10_shot_query_instacrop_3_3.5_new_scheduler/last_epoch.pth \\\n",
    "   --test_time_aug_samples 2 --query_aug instance'.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-FS dataset - phase val\n",
      "CIFAR_FS(phase=val, augment=null)\n"
     ]
    }
   ],
   "source": [
    "if opt.wandb:\n",
    "    wandb.init(project=\"330proj\", entity=\"erictang000\")\n",
    "\n",
    "valset = get_datasets(opt.dataset, 'val', opt)\n",
    "\n",
    "dloader_val = FewShotDataloader(valset, kway=opt.test_way, kshot=opt.val_shot, kquery=opt.val_query,\n",
    "                                batch_size=1, num_workers=1, epoch_size=opt.val_episode, shuffle=False, fixed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu: 0\n",
      "Features dimension: 32\n"
     ]
    }
   ],
   "source": [
    "set_gpu(opt.gpu)\n",
    "\n",
    "# Check whether or not to initialize instance augmentation module:\n",
    "if not opt.load:\n",
    "    print(\"Checkpoint file required\")\n",
    "    exit(0)\n",
    "state_dict = torch.load(opt.load)\n",
    "\n",
    "Li_configs={'li_flag': False}\n",
    "if (opt.support_aug and \"instance\" in opt.support_aug) or (opt.query_aug and \"instance\" in opt.query_aug) or (opt.shot_aug and \"instance\" in opt.shot_aug):\n",
    "    # initialize instance augmentation module\n",
    "\n",
    "    if opt.Li_config_path:\n",
    "        Li_configs=yaml.safe_load(open(opt.Li_config_path,'r'))\n",
    "    else:\n",
    "        print(\"instance module specified, but config not provided\")\n",
    "        exit(0)\n",
    "    instance_aug_module = learnable_invariance(Li_configs).to(DEVICE)\n",
    "    instance_aug_module.load_state_dict(state_dict[\"li\"])\n",
    "    instance_aug_module.eval()\n",
    "\n",
    "\n",
    "if (opt.support_aug and \"random_crop\" in opt.support_aug) or (opt.query_aug and \"random_crop\" in opt.query_aug) or (opt.shot_aug and \"random_crop\" in opt.shot_aug):\n",
    "    random_crop = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(32, scale=[0.8, 1]),\n",
    "    ])\n",
    "state_dict = torch.load('./experiments/CIFAR_5_shot_query_cutmix/epoch_40.pth')\n",
    "(embedding_net, cls_head) = get_model(opt)\n",
    "embedding_net = embedding_net.to(DEVICE)\n",
    "embedding_net.load_state_dict(state_dict[\"embedding\"])\n",
    "cls_head = cls_head.to(DEVICE)\n",
    "cls_head.load_state_dict(state_dict[\"head\"])\n",
    "\n",
    "embedding_net.eval()\n",
    "cls_head.eval()\n",
    "\n",
    "max_val_acc = 0.0\n",
    "\n",
    "timer = Timer()\n",
    "x_entropy = torch.nn.CrossEntropyLoss()\n",
    "x_entropy_Li = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate on the validation split\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "for i, batch in enumerate(dloader_val(opt.num_epoch), 1):\n",
    "    data_support, labels_support, _, data_query, labels_query, _ = [x.to(DEVICE) for x in batch]\n",
    "\n",
    "    test_n_support = opt.test_way * opt.val_shot\n",
    "    test_n_query = opt.test_way * opt.val_query\n",
    "\n",
    "    for method in opt.shot_aug:\n",
    "        if method == \"instance\":\n",
    "            data_support, labels_support, test_n_support, _, _ = shot_aug(data_support, labels_support, test_n_support, method, opt, instance_aug_module)\n",
    "        elif method == \"random_crop\":\n",
    "            data_support, labels_support, test_n_support = shot_aug(data_support, labels_support, test_n_support, method, opt, random_crop=random_crop)\n",
    "        else:\n",
    "            data_support, labels_support, test_n_support = shot_aug(data_support, labels_support, test_n_support, method, opt)\n",
    "    # ## test time augmentation\n",
    "    data_query_aug_all = []\n",
    "    if len(opt.query_aug) == 0:\n",
    "        data_query_aug_all = [data_query]\n",
    "    else:\n",
    "        for mi, query_method in enumerate(opt.query_aug):\n",
    "            ## instance augmentation\n",
    "            data_query_aug_all.append(data_query)\n",
    "            if \"instance\" in query_method:\n",
    "                for j in range(opt.test_time_aug_samples - 1):\n",
    "                    e, b = data_query.shape[0], data_query.shape[1]\n",
    "                    data_query_aug = data_query.view((-1, data_query.shape[2], data_query.shape[3], data_query.shape[4]))\n",
    "                    data_query_aug, _, _, _ = apply_instance_aug(instance_aug_module, data_query_aug)\n",
    "                    data_query_aug = data_query_aug.view((e, b, data_query.shape[2], data_query.shape[3], data_query.shape[4]))\n",
    "                    data_query_aug_all.append(data_query_aug)\n",
    "            elif \"random_crop\" in query_method:\n",
    "                for j in range(opt.test_time_aug_samples - 1):\n",
    "                    e, b = data_query.shape[0], data_query.shape[1]\n",
    "                    data_query_aug = data_query.view((-1, data_query.shape[2], data_query.shape[3], data_query.shape[4]))\n",
    "                    data_query_aug = random_crop(data_query_aug)\n",
    "                    data_query_aug = data_query_aug.view((e, b, data_query.shape[2], data_query.shape[3], data_query.shape[4]))\n",
    "                    data_query_aug_all.append(data_query_aug)\n",
    "    data_query_aug_all = torch.cat(data_query_aug_all, dim=0)\n",
    "\n",
    "    emb_support = embedding_net(data_support.reshape([-1] + list(data_support.shape[-3:])))\n",
    "    emb_support = emb_support.reshape(1, test_n_support, -1)\n",
    "    emb_query = embedding_net(data_query_aug_all.reshape([-1] + list(data_query_aug_all.shape[-3:])))\n",
    "    emb_query = emb_query.reshape(1, opt.test_time_aug_samples * test_n_query, -1)\n",
    "\n",
    "    logit_query = cls_head(emb_query, emb_support, labels_support, opt.test_way, opt.val_shot * opt.s_du)[0]\n",
    "    if opt.test_time_aug_samples > 1:\n",
    "        logit_query = logit_query.view(opt.test_time_aug_samples, test_n_query, -1).sum(0)\n",
    "        \n",
    "    loss = x_entropy(logit_query.reshape(-1, opt.test_way), labels_query.reshape(-1))\n",
    "    acc = count_accuracy(logit_query.reshape(-1, opt.test_way), labels_query.reshape(-1))\n",
    "\n",
    "    val_accuracies.append(acc.item())\n",
    "    val_losses.append(loss.item())\n",
    "    \n",
    "val_acc_avg = np.mean(np.array(val_accuracies))\n",
    "val_acc_ci95 = 1.96 * np.std(np.array(val_accuracies)) / np.sqrt(opt.val_episode)\n",
    "\n",
    "val_loss_avg = np.mean(np.array(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.62266840171814"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39235224935983715"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_ci95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_query_aug[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f417c5b3b20>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAYJ0lEQVR4nO3dfXBV9ZkH8O9D5DUBQkyIEZGIxBeqFmlAUVbxddQ6g3Z3rP7h2K2Vzm6dXXe6M8vYWV863dburHactusWVy22vqAiylLfrZaqXTAgr6IIMSgQIAEiJLyGPPvHOcwE9zxPLufmnhv8fT8zDOH35HfPj5P73HNznvv7/URVQURfff2KPQAiygaTnSgQTHaiQDDZiQLBZCcKBJOdKBDH5dNZRK4C8CCAEgD/rar3ed9fWTlCa2tHJQe7Os1++3d3JLbv2LHL7LNzx1778WCXG7vMCABIYusA5zSWlpaasX797NfavXv3mbE9ne1mzHr9Hlgy0OwxomyQGTt+SH8ztnfbFjO261By+wGzB1A3aqgdPMF43gAA7HMcmqamJrS2tiY+UVMnu4iUAPg1gCsAbATwvojMV9UPrT61taPQ0DA3Odix1TzWp396P7H9yd+/YfZ59tkV9uN1Gs9EAPbLByBGItWg2uwz5exJZmzI4MFmbMXKj8xYQ+u7Zsx64o8pH2v2uH7KeDP2txNHmrFlv7Zf21/fntzebPYA5t8+2YyVzPw3p+d5TuyrKvk5XF9vn4t83sZPBrBOVRtV9QCApwFMz+PxiKiA8kn2UQA+7/bvjXEbEfVBBb9BJyIzRKRBRBpaWnYW+nBEZMgn2TcBGN3t3yfFbUdQ1VmqWq+q9VVVI/I4HBHlI59kfx9AnYicIiIDANwIYH7vDIuIelvqu/Gq2ikitwN4FVHp7VFVXe13AmBV2EqHm91OqU++Wzz9kFO6KjnejL29sMmMrf6sxYy1Ibkc1m6U5ABg/wG7dFU2zC4ZDR7mlKFa7RCwJ7F17fYms8eCBRvN2HGv2b96HXDqaOVG+xS7C0ouvtSJnuTEPFYx9Vj/iEnJUffIq86uqi8BeCmfxyCibBzrL29ElCMmO1EgmOxEgWCyEwWCyU4UiLzuxh+tPW0dWDJ3UWJs9Jgys1+//gcT2w/CnslVOqLcjA2tGmbGBm1yZpQdSq41bYfd58OPG83YiFZ7IkzLDntGGYwSYMQqydgTjVY7swC/cMprdc4ozjfaz7YrkcCItOW1T+1Q4/Lkdm+G3cAznWOdmNOI+iJe2YkCwWQnCgSTnSgQTHaiQDDZiQKR6d345s3b8ZN7H0+MnTl+dGI7AJRXDEls371vv9ln1Vr7bnbztuTJIgCw75A9qQXmXWt7ssjHHclLagEAOuzlsfw77l4/a4zptvmyp8jYc5oAwFoEqzm5sAIAaF28zoxVDqi0O65eY4YafzMrsX3sRGedlX/5nh0rvd6Owa6u9AW8shMFgslOFAgmO1EgmOxEgWCyEwWCyU4UCFFNV5JJdTAR82D9Ya88OxwVRh+71LHbKQx5myf5JSqrbuQ9orfHjFde6/u8K8UYo/00p09djb0N1fjzJpqx1nffM2NvGUsKXuKM41/vso+Fex90ek51Ytmor69HQ0NDYv2YV3aiQDDZiQLBZCcKBJOdKBBMdqJAMNmJApHXrDcRaQKwG9E0rE5VrU/7WAedklerEfPnp3klNG++ljejzOrnjeTYNsmJ2fMU7aLiZu9gzXYpctcLdnnN3rALWGW0e8+Oy3+81IxNqb7L7vj3P3Yetfhlud6Y4nqJqrq7jxFR8fFtPFEg8k12BfCaiCwRkRm9MSAiKox838ZPVdVNIjISwOsi8pGqLuz+DfGLAF8IiIosryu7qm6K/94GYB6AyQnfM0tV6/O5eUdE+Uud7CJSKiJDD38N4ErYNz+JqMjyeRtfDWCeiBx+nCdV9RW/iziHtLdyAgYktqq51REAOPsWocuJOSsiwlrgMs1MuWPD1JpyM3Z1WZsZ2/1JcvsC51je4pabnNheJzbSaPfmGyZvUBYZ96O3zFhV/5/aHW/7mfOoX3divSd1sqtqI7IaJRHljaU3okAw2YkCwWQnCgSTnSgQTHaiQGS611s/9MMQJO/b1u6W0Sze7DWvvJa29GYVbLzH82bR9X3Hj7QXAr3iH66zO3Ykz2+r+tlrZpffNdsPt9YOwV6mEhhntHs/sfVO7Ok2Ozbln142Y/U159gdr03OCaDOGcnR45WdKBBMdqJAMNmJAsFkJwoEk50oEJneje+PEpyI4YmxRuwx+3Wiw4hYdzEBuHf3vTvk3npyVj9vKsax7e3ln5qxSz+w79RPuemixPa/esLeAKr0N3PN2H/OsW/VrzYj9k/sLKfPVifmTev8wnqaAtDH7P/bpLMuTg7U8m48EaXAZCcKBJOdKBBMdqJAMNmJAsFkJwpEpqW3srJBmFr/tcTYvoVrzH6fdTk1DVN/J+ZNg/B8dbd5srzhxNb+yt4m6bKXk2NTzzvJ7FO+u92MeT8xbz05q3BoTZABgDIn5q02uMuJtXU4K+yVpHl+Hz1e2YkCwWQnCgSTnSgQTHaiQDDZiQLBZCcKRI+lNxF5FMC1ALap6llxWwWAOQBqATQBuEFVd/b0WCMqhuJbNyXP8NnTz37dOfRe8gpkm/Z5xY60a7+lLcuF5zMn9pixkNtj6+0SlDH3CwDgPbm8Z4H1BPd+yhVObLAT8wpoe8UpEA7uMXV6RS5X9t8CuOpLbTMBvKmqdQDejP9NRH1Yj8ke77e+40vN0wHMjr+eDeC63h0WEfW2tL+zV6vq4dUEtiDa0ZWI+rC8b9CpqsL5FKGIzBCRBhFp+KI9m48FEtH/lzbZt4pIDQDEf2+zvlFVZ6lqvarWDy8rTXk4IspX2mSfD+CW+OtbALzYO8MhokLJpfT2FIBpACpFZCOAuwHcB+AZEbkVwAYAN+RysOGVw/HN73z5xn6kc7B91e8akLzBzyuvNJh9djkLWPrSbhtF+fpTyn7JS5hGkudYAvZSmcAJTmy/EzPf3gI4cMAJYoPR7i1k6hUBk/WY7Kp6kxG67KiPRkRFw0/QEQWCyU4UCCY7USCY7ESBYLITBSLTBSchA4EByftXTb95pNmttKImsX3ffnv4f3jrPTN2yJ0R55XX+NrYFw1wYsnPHGCU02eYEzvoxLydByu9DeQ2Ggt3VlolOQA4w4kl47OXKBBMdqJAMNmJAsFkJwoEk50oEEx2okBkW3oDYO+XZhVJgMu/eUFi+5bNu80+a1Y2mbFPWrebMaSeLUfF4u3NZu345y1SebwTq3Ri3j5w+qETXPBycvuES51O1p55dumYV3aiQDDZiQLBZCcKBJOdKBBMdqJAFOFuvHfP0pJ8p/7KKyeZPZ7//R/M2PaF9l38He592jRjp0LzJqdYmy61On1OdWKVzgLJ6qyUvsV56jTNS26vrfuV3enb04yAvc0Ur+xEgWCyEwWCyU4UCCY7USCY7ESBYLITBSKX7Z8eBXAtgG2qelbcdg+A2wC0xN92p6q+1PPhJJdD5mzkmJPN2DfOPcWMfbxsrRlr22Vv8KPGtIpSJG9PBQDt2GnGqHdsdGJWie1Ep0+JExtsL5WI/U49r92u9uIzYwm66tn2GnSDL/xjcuCAXTrO5cr+WwBJG7T9QlUnxH9ySHQiKqYek11VFwLYkcFYiKiA8vmd/XYRWSEij4qItykmEfUBaZP9IUSfKpwAoBnA/dY3isgMEWkQkYaWFu9DikRUSKmSXVW3quohVe0C8DCAyc73zlLVelWtr6ry1vkgokJKlewi0n1myvUAVvXOcIioUHIpvT0FYBqAShHZCOBuANNEZAKiaWBNAL6f+yF7c+bYUDMybdq5ZmzpX1aasV2L28zYAEkusY0eYxdyjq+0b2es/XCdGVu1xx4j5e5To/08p48MtGPtdmXWLa95M/OsjNj0tt1n3LynkgNt9r30HpNdVW9KaH6kp35E1LfwE3REgWCyEwWCyU4UCCY7USCY7ESByHjBSQXQacScekcKF15il96WLLbLWh3t9vZPn29I3jaqbaddcxk0aIgZ69fP2gqLeov1AZAvnD5tTp3swGY75s2+837SVgF5515nHMuWJbars3sZr+xEgWCyEwWCyU4UCCY7USCY7ESBYLITBaIIpbdDTsySokQ1vM4MXXP9VDPW3mHXLl5dsCixfV3jNrPP9i/sBQC3o9mMfVV5V5euzEYBfO7E1jsDsZ69ALDGidlLo9qx450+7U3J7YcO2H14ZScKBJOdKBBMdqJAMNmJAsFkJwpExnfjAfueqzVBBoCx7ZLP3sRn3KQLzdj0g/YiY5s3J991X9u4ye6DRjMWoiuc2KuZjQJuHeRjJ+YsM4f3ndg3nFip0V7t9Gk3nlZdzhp5vLITBYLJThQIJjtRIJjsRIFgshMFgslOFIhctn8aDeBxRJUABTBLVR8UkQoAcwDUItoC6gZV3ek/WheAfUbMK6+lKb15hpuRr10wxYzdvDu58LJ6aZPZZ0ujt7V9hxP7avImhNhnHuhyLksnjrVjdeXJ7Xuciugwp77W7qxP12aHsNaJlRvtXjF6Q1Nyu7e2Xi5X9k4AP1TV8QDOB/ADERkPYCaAN1W1DsCb8b+JqI/qMdlVtVlVl8Zf70Y0k28UgOkAZsffNhvAdQUaIxH1gqP6nV1EagGcC2ARgGpVPfxBpC3wP/BDREWWc7KLSBmAuQDuUNUjVmRQVYWx+oSIzBCRBhFpaGlJXnediAovp2QXkf6IEv0JVX0+bt4qIjVxvAZA4gfHVXWWqtaran1Vlbf2BhEVUo/JLiKCaD/2Nar6QLfQfAC3xF/fAuDF3h8eEfWWXGa9XQjgZgArRWRZ3HYngPsAPCMitwLYAOCGnh9KAVi1C6emYRYhCjFpr9yMnHfxqYntZ0+0C0orGreasR3uSmje+Th2easJeu/7qu0lBXHRt8ebsSumTEtsf/PJBWafLz76zIypM7Vtoh1CqxNbb7RvcPpYS815te8es0VV34H9M7qsp/5E1DfwE3REgWCyEwWCyU4UCCY7USCY7ESB6EMLTnob61hlKHtRyVRbRgFwX/8GVSU2T51ml36WvmMXXd7d4s1r2uLEnD1++jhvVpancogdO/2Ms8xYzeTkuXSlb9jz0D5eZpfePrWH4Za97CVJ7cUvy5w+yc9EfxM1XtmJAsFkJwoEk50oEEx2okAw2YkCwWQnCkTGpTdxDumVyqxynVdoSFt68x5zWGLr1deeb/b44M9tZqx5zl4z1mguzAkYSwccE7zS1Qgn1s85HXva7BmCHeuSZxY2fW4vpPKXVfax3rVD7k8sDbugC1xltHs7C/LKThQIJjtRIJjsRIFgshMFgslOFIgi3I0fYMTSbP9UiNeqgU4seWn8oWPsjYu+8909ZuyAsZ0UADz3kjU9ArCnafR9i53YVCf20Ro7tvOheWbsg9r/SWyvqDzT7HPjDHsk5fPeMWNze7lIMsaJXWC0P+704ZWdKBBMdqJAMNmJAsFkJwoEk50oEEx2okD0WHoTkdGI7uhXI5olMktVHxSRewDcBqAl/tY7VfWlng9pTTTxJqB4sSxZk2uGmj1Ov9LeFOjWA/Zacv0H2uvTzZlnr9fXdAwX5uxNtICRziyZjnV2bPXK5PP41zPssuew0+y9pprOPdE+2Kub7ZjDuuLWOn2Sp2T5qzLmUmfvBPBDVV0qIkMBLBGR1+PYL1T1P3J4DCIqslz2emtGvACmqu4WkTUARhV6YETUu47qd3YRqQVwLoBFcdPtIrJCRB4VEW86MhEVWc7JLiJlAOYCuENVdwF4CMCpACYguvLfb/SbISINItLQ0mIvGEBEhZVTsotIf0SJ/oSqPg8AqrpVVQ+paheAhwFMTuqrqrNUtV5V66uqvB24iaiQekx2EREAjwBYo6oPdGuv6fZt1wNwFvMhomLL5W78hQBuBrBSRJbFbXcCuElEJiCqizUB+H7PD9UFYL8R82a9pSnXpV2DLg1v66oaMzLeWbvue4Ps1+H9as+km/dC8kpoG46BdevKnViJs7eSvZKfvS7cgNYms8+w8ePM2Mhqex8q76aVtzWU9X7Xezxr7qj3rM/lbvw7xmPkUFMnor6Cn6AjCgSTnSgQTHaiQDDZiQLBZCcKRMYLTnbBLpR4Cz0ONtrTbBkFpH+Ns2ai2TOogDInNtqMjLu81IzdDrv0Ngjtie0vvPCq2ecjt3SYnZaUsQ4nZhXROj+x+wyctNWMVVdYRS9/lppXerM2r/J+Kmk2UeOVnSgQTHaiQDDZiQLBZCcKBJOdKBBMdqJAZFx6A+zylVcqS8MqaADpZ8RZY7dLYf7/yyopAoA9u+rUy62dvoCZpcn/t7qT7bUE5jz5mhl7u9UuQ9nLZabzvBPzylB2kRK43Gg/5wS7z4kXTzJjp/erMmPrln9uxj54yy7PWs8qb6kX66fiPet5ZScKBJOdKBBMdqJAMNmJAsFkJwoEk50oEEUovVmvL72915v3OubFvGNZBSCvMGTv2ebHvAU47fLP8ClTEtu/W233Of3s08zYxPl/NmOvv2zHlnR6y0AmSzv3zpv1ZhVFO5MnB0YOOs+PEfa+fuNGj3Ee1F582SoE73IezSq9ec8oXtmJAsFkJwoEk50oEEx2okAw2YkC0ePdeBEZBGAhokXijgPwnKreLSKnAHga0e41SwDcrKo9zI0QACUphmndp/UmmXjH8V7j0qxd551GL5Z2jIOc2MnJzWMrzR4XjrXvIp95Tq0Zm3R+nRl746V3Ettfe3e52We9GUlvndG+Zq3dZ8j/2gvUHaiy73fv3+6tDWizVrXz7qxbR/IqGrlc2fcDuFRVv45oe+arROR8AD8H8AtVHYdoPb1bc3gsIiqSHpNdI4erkv3jPwrgUgDPxe2zAVxXiAESUe/IdX/2kngH120AXkf0jqtNVQ+/09gIYFRBRkhEvSKnZFfVQ6o6AcBJACYDOCPXA4jIDBFpEJGGlpYd6UZJRHk7qrvxqtoG4C0AUwCUi8jhu08nAdhk9JmlqvWqWl9VVZHPWIkoDz0mu4hUiUh5/PVgAFcAWIMo6f8m/rZbALxYoDESUS/IZSJMDYDZIlKC6MXhGVVdICIfAnhaRH4C4AMAj+Q3lDQTULwVt7yJJB7v9c8qkniFFS/mbXmVtjxoTasY7vSxV3GrmFxuxr41+Rw7dt20xPbnn/ij2eeXP33IjL1tRnxWhW2Js8DbZy8uNmP7hljFPKDp/cQ3twD8Z7f1k7FXIUw3NazHZFfVFQDOTWhvRPT7OxEdA/gJOqJAMNmJAsFkJwoEk50oEEx2okCIapqb+CkPJtICYEP8z0oArZkd3MZxHInjONKxNo4xqpq44GCmyX7EgUUaVLW+KAfnODiOAMfBt/FEgWCyEwWimMk+q4jH7o7jOBLHcaSvzDiK9js7EWWLb+OJAlGUZBeRq0TkYxFZJyIzizGGeBxNIrJSRJaJSEOGx31URLaJyKpubRUi8rqIfBL/PaJI47hHRDbF52SZiFyTwThGi8hbIvKhiKwWkX+M2zM9J844Mj0nIjJIRBaLyPJ4HPfG7aeIyKI4b+aIiDUNM5mqZvoH0ZKq6wGMRTRndDmA8VmPIx5LE4DKIhz3IgATAazq1vbvAGbGX88E8PMijeMeAP+c8fmoATAx/nooopmp47M+J844Mj0niOYpl8Vf9wewCMD5AJ4BcGPc/l8A/u5oHrcYV/bJANapaqNGS08/DWB6EcZRNKq6EMCX1+iajmjhTiCjBTyNcWROVZtVdWn89W5Ei6OMQsbnxBlHpjTS64u8FiPZRwH4vNu/i7lYpQJ4TUSWiMiMIo3hsGpVbY6/3gKguohjuV1EVsRv8wv+60R3IlKLaP2ERSjiOfnSOICMz0khFnkN/QbdVFWdCOBqAD8QkYuKPSAgemVHusVIesNDAE5FtEdAM4D7szqwiJQBmAvgDlU9YsfiLM9JwjgyPyeaxyKvlmIk+yYAo7v921ysstBUdVP89zYA81DclXe2ikgNAMR/byvGIFR1a/xE6wLwMDI6JyLSH1GCPaGqz8fNmZ+TpHEU65zEx27DUS7yailGsr8PoC6+szgAwI0A5mc9CBEpFZGhh78GcCWAVX6vgpqPaOFOoIgLeB5Ortj1yOCciIggWsNwjao+0C2U6TmxxpH1OSnYIq9Z3WH80t3GaxDd6VwP4EdFGsNYRJWA5QBWZzkOAE8hejt4ENHvXrci2jPvTQCfAHgDQEWRxvE7ACsBrECUbDUZjGMqorfoKwAsi/9ck/U5ccaR6TkBcA6iRVxXIHphuavbc3Yxoi3sngUw8Ggel5+gIwpE6DfoiILBZCcKBJOdKBBMdqJAMNmJAsFkJwoEk50oEEx2okD8H04lCoJHbP0IAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(data_query_aug_all[5][0,1].detach().cpu().numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f417c7c16d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(data_query[0,1].detach().cpu().numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1ab61a329566cc13b7878ae693f1437cd38df5189cbdd89ad8e228bdbc8c6fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
